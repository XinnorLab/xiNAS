# xiRAID API Behavior Documentation

**Generated by:** xiraid-analyst (read-only)
**Source tree:** `/home/xinnor/eraraid-us-master`
**Date:** 2026-02-26
**Target product:** xiNAS-MCP (Node.js/TypeScript MCP server)

---

## 1. Overview

xiRAID exposes **two** internal servers. Only one is relevant for xiNAS-MCP.

| Server | Protocol | Address | Purpose |
|---|---|---|---|
| **gRPC Management API** | gRPC over TLS | `localhost:6066` (default) | All RAID/drive/pool/settings management — **use this** |
| Config Server | JSON over TCP | `localhost:14088` | Internal config caching daemon — **do not use externally** |

The gRPC address is read from `/etc/xraid/net.conf` (JSON: `{"host": "...", "port": ...}`). The default is `localhost:6066`.

**Critical finding:** No separate helper daemon is required. xiNAS-MCP can connect directly to the gRPC API using generated TypeScript/Node.js stubs from the `.proto` files.

---

## 2. Public Surface

### 2.1 Proto Package

Package name: `xraid.v2`
Service name: `XRAIDService`
Proto source: `src/usr/lib/xraid/gRPC/protobuf/service_xraid.proto`

All message definitions are in:

| File | Namespace |
|---|---|
| `message_raid.proto` | Raid*, RaidInit*, RaidRecon*, RaidRestripe*, RaidImport* |
| `message_drive.proto` | Drive* |
| `message_pool.proto` | Pool* |
| `message_settings.proto` | Settings* |
| `message_config.proto` | Config* |
| `message_license.proto` | License* |
| `message_cluster.proto` | Cluster* |
| `message_log.proto` | Log* |
| `message_mail.proto` | Mail* |
| `message_sdc.proto` | SDC* |

### 2.2 Full RPC Table

Every RPC returns `ResponseMessage { optional string message = 1; }`.

`message` is a JSON string (occasionally plain text or empty). Parse with `JSON.parse()` and catch `SyntaxError` for non-JSON responses.

#### RAID operations

| RPC | Message | Mutates | Notes |
|---|---|---|---|
| `raid_show` | `RaidShow` | No | Returns JSON array of RAID info |
| `raid_create` | `RaidCreate` | Yes | Synchronous — no job_id returned |
| `raid_destroy` | `RaidDestroy` | Yes | Irreversible data loss |
| `raid_modify` | `RaidModify` | Yes | Live parameter changes |
| `raid_unload` | `RaidUnload` | Yes | Safe — data preserved |
| `raid_restore` | `RaidRestore` | Yes | Restores from config file |
| `raid_import_show` | `RaidImportShow` | No | Shows importable RAIDs from drive metadata |
| `raid_import_apply` | `RaidImportApply` | Yes | Imports RAID by UUID |
| `raid_replace` | `RaidReplace` | Yes | Drive replacement in RAID |
| `raid_resize` | `RaidResize` | Yes | Resizes RAID (no new params needed) |
| `raid_defaults_show` | `RaidDefaultsShow` | No | Default settings per level/drive type |
| `raid_init_start` | `RaidInitStart` | Yes | Resume initialization |
| `raid_init_stop` | `RaidInitStop` | Yes | Pause initialization |
| `raid_init_force_finished` | `RaidInitForceFinished` | Yes | Pseudo-complete init (dangerous) |
| `raid_init_reset` | `RaidInitReset` | Yes | Reset init progress |
| `raid_recon_start` | `RaidReconStart` | Yes | Resume reconstruction |
| `raid_recon_stop` | `RaidReconStop` | Yes | Pause reconstruction |
| `raid_recon_force_finished` | `RaidReconForceFinished` | Yes | Pseudo-complete recon (dangerous) |
| `raid_restripe_start` | `RaidRestripeStart` | Yes | Start level/topology change |
| `raid_restripe_continue` | `RaidRestripeContinue` | Yes | Resume restripe |
| `raid_restripe_stop` | `RaidRestripeStop` | Yes | Pause restripe |
| `raid_restripe_force_finished` | `RaidRestripeForceFinished` | Yes | Pseudo-complete restripe |

#### Drive operations

| RPC | Message | Mutates | Notes |
|---|---|---|---|
| `drive_faulty_count_show` | `DriveFaultyCountShow` | No | Per-drive I/O error counters |
| `drive_faulty_count_reset` | `DriveFaultyCountReset` | Yes | Resets error counters |
| `drive_locate` | `DriveLocate` | Yes | Toggle LED; empty drives list = off |
| `drive_clean` | `DriveClean` | Yes | Wipes metadata + resets fault counter |

#### Spare pool operations

| RPC | Message | Mutates | Notes |
|---|---|---|---|
| `pool_show` | `PoolShow` | No | Lists pools and member drives |
| `pool_create` | `PoolCreate` | Yes | Creates pool with initial drives |
| `pool_delete` | `PoolDelete` | Yes | Destroys pool |
| `pool_add` | `PoolAdd` | Yes | Adds drives to existing pool |
| `pool_remove` | `PoolRemove` | Yes | Removes drives from pool |
| `pool_activate` | `PoolActivate` | Yes | Enables auto-replace |
| `pool_deactivate` | `PoolDeactivate` | Yes | Disables auto-replace |
| `pool_acquire` | `PoolAcquire` | Yes | Internal: claims a drive from pool |

#### Settings

| RPC | Message | Notes |
|---|---|---|
| `settings_auth_show/modify` | `SettingsAuthShow/Modify` | gRPC host/port configuration |
| `settings_faulty_count_show/modify` | `SettingsFaultyCountShow/Modify` | I/O error threshold (default 3) |
| `settings_log_show/modify` | `SettingsLogShow/Modify` | Log level and directory |
| `settings_scanner_show/modify` | `SettingsScannerShow/Modify` | Polling intervals, LED behaviour |
| `settings_cluster_show/modify` | `SettingsClusterShow/Modify` | `raid_autostart`, `pool_autoactivate` |
| `settings_pool_show/modify` | `SettingsPoolShow/Modify` | Replace delay (seconds) |
| `settings_mail_show/modify` | `SettingsMailShow/Modify` | Mail settings (deprecated) |

#### Config, License, Log, SDC

| RPC | Notes |
|---|---|
| `config_backup` | Creates `/etc/xraid/backup_raid.conf` at current directory |
| `config_restore` | Restores from file or drive metadata |
| `config_show` | Shows configs stored on drives |
| `config_apply` | Applies config file to restoring RAIDs |
| `license_show/update/delete` | License lifecycle |
| `log_show` | Last N event log entries |
| `log_collect` | Bundles event log into `.tar.gz` |
| `sdc_start/stop/reset` | Silent Data Corruption scanner per RAID |

---

## 3. Behavior by Module

### 3.1 Connection & Authentication

**Source:** `gRPC/xraid_client.py:XNRgRPCClient.authentication()`, `gRPC/auth.py`, `gRPC/constant.py`

- Transport: `grpc.secure_channel` with TLS
- Auth type: **one-way TLS** — client validates server using CA cert; server does not request client cert
- CA cert file: `/etc/xraid/crt/ca-cert.{pem,crt}` (first match wins)
- gRPC target: read from `/etc/xraid/net.conf` → `{"host": "...", "port": N}` → `"{host}:{port}"`
- HTTP proxy explicitly disabled: `grpc.enable_http_proxy = 0`
- Retry on UNAVAILABLE: 5 attempts, 1-second backoff (`CLI_CONNECTION_ATTEMPTS = 5`)

**For TypeScript MCP client:**
```
credentials = grpc.credentials.createSsl(fs.readFileSync('/etc/xraid/crt/ca-cert.pem'))
channel → grpc.ServiceClient(target, credentials, { 'grpc.enable_http_proxy': 0 })
```

### 3.2 Request → Response Lifecycle

**Source:** `gRPC/xraid_server.py:request_decorator`, `gRPC/response.py`

1. CLI/client serialises arguments into proto message using `json_format.ParseDict(payload, buffer)`
2. gRPC stub sends `buffer` to server
3. Server `request_decorator` runs: validation check → handler function
4. Handler returns data (dict, list, or None)
5. Server calls `fill_resp(resp)` which wraps result as `ResponseMessage(message=json.dumps(data))`
6. Client calls `json.loads(response.message)` — falls back to plain string on `JSONDecodeError`

**Error path:** If `ServerException` is raised in handler, `fill_rc(e, context)` maps it to a gRPC status code with `context.set_details(str(e))` and returns `grpc.StatusCode.INVALID_ARGUMENT` (or `INTERNAL`).

### 3.3 Root Privilege Requirement

**Source:** `gRPC/xraid_client.py:send()`, `core/constant.py:CLI_ROOT_COMMAND`

- All mutating RPCs require root (UID 0)
- Non-root allowed for read-only commands (listed in `CLI_ROOT_COMMAND`): `raid show`, `pool show`, `license show`, `log show`, `drive faulty-count show`, `settings.*show`, `config show`, `raid import show`
- **The MCP server process MUST run as root** or as a user with equivalent privileges via `sudo` or capabilities

### 3.4 RAID Create — Constraints & Preflight

**Source:** `gRPC_server_handler/raid/create.py`, `gRPC/validation/raid.py`, `core/constant.py:ARGUMENT_BOUNDARY_MIN_MAX`

Required fields: `name`, `level`, `drives[]`

Optional field valid ranges:
| Field | Range / Choices |
|---|---|
| `level` | From `RAID_LEVELS` constant (0,1,5,6,7,10,50,60,70,N+M notation) |
| `strip_size` | From `STRIP_SIZES_KB` (powers of 2 in KiB) |
| `block_size` | From `RAID_BS_CHOICE` (512 or 4096) |
| `group_size` | 2–32 (required for levels 50/60/70) |
| `synd_cnt` | 4–32 (for N+M levels) |
| `memory_limit` | 1024–1048576 MiB, or 0 (disabled) |
| `memory_prealloc` | 1024–65536 MiB, or 0 |
| `init_prio`, `recon_prio`, `restripe_prio`, `sdc_prio` | 1–100 |
| `sched_enabled`, `merge_read_enabled`, `merge_write_enabled`, `adaptive_merge` | 0 or 1 |
| `max_sectors_kb` | 4–4096, or 0 |
| `request_limit` | 0–INT_MAX |

**Server-side preflight errors thrown:**
- `RAIDMinimumDrivesRequiredError` — insufficient drives for level
- `RAIDNeedToSpecifyGroupSizeError` — group_size mandatory for 50/60/70
- `RAIDDefaultGroupSizeNotApplicableError` — cannot use default group_size
- `RAIDIncorrectGroupSizeError` — group_size invalid for level
- `RAIDGroupsNumberTooSmallError` — need ≥ 2 groups for compound levels
- `RAIDNumberDevicesNotDividedByGroupSizeError` — drive count not divisible by group_size
- `RAIDMempoolTooLargeError` — not enough free RAM for requested memory_prealloc
- `RAIDMergeFor1MError` — merge_write enabled but full stripe > 1 MiB

**`force=true` field:** Overrides some safety guards (e.g., `force_metadata` to overwrite existing drive metadata). Expose only to `admin` role.

### 3.5 RAID Destroy — Constraints

**Source:** `gRPC_server_handler/raid/destroy.py`, `exceptions/raid.py:RAIDStateNotInExpectedError`

- Requires `name` (specific) or `all=true` (all RAIDs)
- `config_only=true`: removes from config without kernel unload (used for import conflict resolution)
- `force=true`: bypasses some state checks
- Server checks RAID is in a state that allows destruction
- **No filesystem/NFS dependency check is performed by xiRAID itself** — the MCP server's plan-mode preflight MUST check this before calling `raid_destroy`

### 3.6 RAID Modify — Live Parameters

**Source:** `gRPC_server_handler/raid/modify.py`, `gRPC_server_handler/raid/helpers/modify_params_ks.py`

Parameters modifiable at runtime without restarting RAID:
- `sched_enabled`, `merge_read_enabled`, `merge_write_enabled`, `adaptive_merge`
- `merge_read_max`, `merge_read_wait`, `merge_write_max`, `merge_write_wait`
- `memory_limit`, `memory_prealloc`, `request_limit`, `max_sectors_kb`
- `cpu_allowed` (CPU affinity mask)
- `init_prio`, `recon_prio`, `restripe_prio`, `sdc_prio`
- `force_online`, `force_resync` (dangerous)
- `discard`, `discard_ignore`, `discard_verify`, `drive_write_through`

Error `RAIDParametersModifyError` is raised if a parameter cannot be modified (e.g., sysfs write fails).

### 3.7 Init / Recon / Restripe Lifecycle

**Source:** `gRPC_server_handler/raid/init.py`, `recon.py`, `restripe.py`

Each background process (init, recon, restripe) has the same lifecycle:
```
start → [running] → stop → [paused] → start (resumes)
                  → force_finished (pseudo-complete, dangerous)
```
- `raid_init_reset`: resets init to 0% (re-initializes)
- `force_finished` requires `force=true` to confirm intent
- Attempting to start recon during restripe (or vice versa) raises `RAIDInProhibitedStateError`

### 3.8 Spare Pool Lifecycle

**Source:** `gRPC_server_handler/pool.py`, `spare_pool/manager.py`

```
create → activate → [monitors RAID] → drive fails → auto-replace
deactivate → [no auto-replace] → delete
```
- Pool must be activated for auto-replace to work
- `pool_acquire` is called internally by the spare pool manager — not for external use
- Replace delay default: 180 seconds (configurable via `settings_pool_modify`)

### 3.9 Drive Locate (LED)

**Source:** `gRPC_server_handler/drive.py`, `service/scanner/v2/led.py`

- `DriveLocate { repeated string drives = 1; }`
- Pass a list of block device paths to light their LEDs
- Pass an **empty list** to turn off all LEDs
- LED support is hardware-dependent; scanner must have LED detection enabled

### 3.10 Config Server (Internal — Do Not Expose)

**Source:** `core/configs/server.py:FileUnifiedConfigurationKeeper`

- TCP on `localhost:14088`
- Auth: random token at `/etc/xraid/crt/config_token.conf` — regenerated on each service start
- Methods: `read`, `write`, `delete`, `snapshot`
- Used internally by all xiRAID components to read/write config files atomically
- **Do not expose this to xiNAS-MCP** — use the gRPC API only

---

## 4. Error Reference

### 4.1 gRPC Status Codes

The server maps `ServerException` subtypes to gRPC status codes via `gRPC/response.py:fill_rc()`. Inspect that file to get the exact mapping. Common patterns observed:

| Condition | Likely gRPC Code |
|---|---|
| Invalid argument value | `INVALID_ARGUMENT` |
| RAID not found | `NOT_FOUND` |
| RAID in wrong state | `FAILED_PRECONDITION` |
| Server-side runtime error | `INTERNAL` |
| Server not reachable | `UNAVAILABLE` (retried 5×) |

### 4.2 Key Exception Classes

| Class | Source | Meaning |
|---|---|---|
| `RAIDStateNotInExpectedError` | `exceptions/raid.py` | Operation not valid in current state |
| `RAIDInProhibitedStateError` | `exceptions/raid.py` | Conflicting background process running |
| `RAIDMinimumDrivesRequiredError` | `exceptions/raid.py` | Too few drives for level |
| `RAIDMempoolTooLargeError` | `exceptions/raid.py` | Insufficient RAM |
| `RAIDMergeFor1MError` | `exceptions/raid.py` | Merge write + stripe size conflict |
| `RAIDRestoreConflictError` | `exceptions/raid.py` | UUID conflict during restore |
| `RAIDInUnloadedState` | `exceptions/raid.py` | Operation on unloaded RAID |
| `RAIDParametersModifyError` | `exceptions/raid.py` | sysfs write failed for parameter |
| `KernelSpaceOperationError` | `exceptions/common.py` | Kernel module operation failed |
| `CommonMessageError` | `exceptions/common.py` | Generic error with optional code |
| `ConfigClientUnauthorized` | `exceptions/config.py` | Config server auth token mismatch |

---

## 5. Concurrency Model

**Source:** `gRPC/xraid_server.py:serve()`, `core/configs/server.py`

- gRPC server: `futures.ThreadPoolExecutor(max_workers=10)` — up to 10 concurrent RPCs
- Config server: per-connection threads via `futures.ThreadPoolExecutor` + per-path write locks (`LockMap`)
- Config server polling loop runs every 1 second to sync filesystem → in-memory cache
- Write lock timeout: 10 seconds (`WRITE_WAITING_TIMEOUT_SECONDS`)
- **No per-RAID operation locking at the gRPC level** — the MCP server must implement this

---

## 6. Performance Notes

- All RPCs are **synchronous** from the caller's perspective — no streaming, no job IDs
- Heavy operations (RAID create, init, recon) execute in the kernel module asynchronously; the RPC returns immediately after triggering the operation
- Progress of init/recon/restripe is readable via `raid_show` (poll with `extended=true`)
- `settings_scanner_modify` controls polling intervals:
  - `scanner_polling_interval`: 1–86400 s (default 1 s)
  - `smart_polling_interval`: 60–86400 s (default 86400 s)
  - `progress_polling_interval`: 1–1440 min (default 10 min)
- Default config server message buffer: 64 KiB (`MAX_MESSAGE_BUFFER = 65536`)

---

## 7. Integration Notes for xiNAS-MCP (TypeScript)

### 7.1 Required Steps — No Helper Daemon Needed

The PRD's "helper daemon" requirement (§8.1) is **resolved**: xiRAID already exposes a full gRPC API. xiNAS-MCP connects directly.

**Steps to integrate:**

1. **Copy `.proto` files** from `src/usr/lib/xraid/gRPC/protobuf/` into the xiNAS-MCP project
2. **Generate TypeScript stubs** using `grpc-tools` or `@grpc/proto-loader`:
   ```
   grpc_tools_node_protoc \
     --js_out=import_style=commonjs,binary:./src/generated \
     --grpc_out=grpc_js:./src/generated \
     -I ./proto \
     service_xraid.proto
   ```
   Or use `@grpc/proto-loader` for dynamic loading (simpler, recommended for TS)
3. **Read connection config** from `/etc/xraid/net.conf` (JSON: `{"host": "...", "port": N}`)
4. **Load CA cert** from `/etc/xraid/crt/ca-cert.pem` (or `.crt`)
5. **Create channel** with `grpc.credentials.createSsl(caCert)` — no client cert needed
6. **Run as root** or ensure the process has read access to `/etc/xraid/crt/`

### 7.2 Response Parsing Pattern

```typescript
// All RPCs return ResponseMessage { message?: string }
const response = await stub.raidShow(request);
try {
  const data = JSON.parse(response.message ?? '{}');
  // data is the RAID array/object
} catch {
  // Plain text response (rare), treat as status string
}
```

### 7.3 Polling for Background Operation Progress

Since RPCs are synchronous triggers (not jobs), use `raid_show` to poll progress:

```typescript
// Poll until init_percent == 100 or state != 'init'
while (true) {
  const resp = await stub.raidShow({ name: raidName, extended: true });
  const data = JSON.parse(resp.message);
  if (data.init_percent === 100) break;
  await sleep(5000);
}
```

### 7.4 MCP Tool → gRPC RPC Mapping

| MCP Tool | gRPC RPC(s) |
|---|---|
| `raid.list` | `raid_show` |
| `raid.create` | `raid_create` (+ `raid_show` to verify) |
| `raid.modify_performance` | `raid_modify` |
| `raid.lifecycle_control` (init) | `raid_init_start` / `raid_init_stop` |
| `raid.lifecycle_control` (recon) | `raid_recon_start` / `raid_recon_stop` |
| `raid.unload` | `raid_unload` |
| `raid.restore` | `raid_restore` |
| `raid.delete` | `raid_destroy` |
| `disk.list` | `raid_show` (extended=true, member drives) + `drive_faulty_count_show` |
| `disk.set_led` | `drive_locate` |
| `disk.secure_erase` | `drive_clean` |
| `pool.list` | `pool_show` |
| `pool.*` | `pool_create/delete/add/remove/activate/deactivate` |
| `system.get_status` | `settings_auth_show`, `settings_scanner_show`, `license_show` + OS APIs |
| `health.run_check` | Compose from `raid_show`, `drive_faulty_count_show`, OS SMART APIs |

### 7.5 What xiRAID Does NOT Provide

The following MCP tool areas require other data sources:

| Need | Source |
|---|---|
| SMART / NVMe health data | `disk.get_smart` — via OS `smartctl` or NVMe sysfs (`/sys/class/nvme/`) |
| NFS share management | Separate NFS helper daemon (see `REQUIREMENTS.md §8.2`) |
| Network interface info | OS sysfs/netlink/ip-command |
| System performance metrics | xiraid-exporter Prometheus endpoint `:9827` |
| Filesystem quota | `xfs_quota` / project quota APIs |
| CPU/RAM/uptime | OS APIs (`/proc/stat`, `/proc/meminfo`) |
